{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/essat20/NLP_CW_210021102/blob/main/NLP_RNN_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d-XCsJ39MTk",
        "outputId": "c4b6a6e3-46e6-4dd0-dfb1-b8d976d8da61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Collecting huggingface-hub>=0.21.2 (from datasets)\n",
            "  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "Successfully installed datasets-2.19.0 dill-0.3.8 huggingface-hub-0.23.0 multiprocess-0.70.16 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "# downloading datasets\n",
        "!pip install datasets # install the datasets from huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1DRHiubPAV2b"
      },
      "outputs": [],
      "source": [
        "# import the libraries\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import DistilBertModel, DistilBertTokenizer, AdamW\n",
        "from datasets import load_dataset\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.optim import AdamW as aw\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "import numpy as np\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "import matplotlib.pyplot as plt # this can be used to display loss/epoch graphs\n",
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "from sklearn.metrics import confusion_matrix # so i can display the confusion matrix\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence  # have to add padding library to ensure all sequences have the same length\n",
        "\n",
        "import seaborn as sb\n",
        "\n",
        "from torch.distributions import Categorical\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # prepares GPU\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset('emotion', trust_remote_code=True) # have to use the second parameter as there is required custom code for the dataset to be loaded properly\n",
        "\n",
        "# Extract text sequences\n",
        "texts = dataset['train']['text']\n"
      ],
      "metadata": {
        "id": "bhrQrkxZB8X1"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_size, num_layers):\n",
        "        super(RNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, input_size)\n",
        "        self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
        "        self.decoder = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input_seq, hidden_state):\n",
        "        embedding = self.embedding(input_seq)\n",
        "        output, hidden_state = self.rnn(embedding, hidden_state)\n",
        "        output = self.decoder(output)\n",
        "        hidden_state = (hidden_state[0].detach(), hidden_state[1].detach())\n",
        "        return output, hidden_state\n"
      ],
      "metadata": {
        "id": "NkLOdwPjCbh0"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    ########### Hyperparameters ###########\n",
        "    hidden_size = 512   # size of hidden state\n",
        "    seq_len = 100       # length of LSTM sequence\n",
        "    num_layers = 3      # num of layers in LSTM layer stack\n",
        "    lr = 0.002          # learning rate\n",
        "    epochs = 3        # max number of epochs\n",
        "    op_seq_len = 200    # total num of characters in output test sequence\n",
        "    save_path = \"charRNN_emotion.pth\"  # Change this to the path where you want to save the model\n",
        "    #######################################\n",
        "\n",
        "    # Load the dataset\n",
        "    dataset = load_dataset('emotion', split='train')  # Load the training split of the emotion dataset\n",
        "\n",
        "    # Extract text sequences\n",
        "    texts = dataset['text']\n",
        "\n",
        "    # Combine all texts into a single string\n",
        "    data = ' '.join(texts)\n",
        "\n",
        "    chars = sorted(list(set(data)))\n",
        "    data_size, vocab_size = len(data), len(chars)\n",
        "    print(\"----------------------------------------\")\n",
        "    print(\"Data has {} characters, {} unique\".format(data_size, vocab_size))\n",
        "    print(\"----------------------------------------\")\n",
        "\n",
        "    # char to index and index to char maps\n",
        "    char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
        "    ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "    # convert data from chars to indices\n",
        "    data = list(data)\n",
        "    for i, ch in enumerate(data):\n",
        "        data[i] = char_to_ix[ch]\n",
        "\n",
        "    # data tensor on device\n",
        "    data = torch.tensor(data).to(device)\n",
        "    data = torch.unsqueeze(data, dim=1)\n",
        "\n",
        "    # model instance\n",
        "    rnn = RNN(vocab_size, vocab_size, hidden_size, num_layers).to(device)\n",
        "\n",
        "    # loss function and optimizer\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adagrad(rnn.parameters(), lr=lr)\n",
        "\n",
        "    # training loop\n",
        "    for i_epoch in range(1, epochs + 1):\n",
        "\n",
        "        # random starting point (1st 10000 chars) from data to begin\n",
        "        data_ptr = np.random.randint(10000)\n",
        "        n = 0\n",
        "        running_loss = 0\n",
        "        hidden_state = None\n",
        "\n",
        "        while True:\n",
        "            input_seq = data[data_ptr: data_ptr + seq_len]\n",
        "            target_seq = data[data_ptr + 1: data_ptr + seq_len + 1]\n",
        "\n",
        "            # forward pass\n",
        "            output, hidden_state = rnn(input_seq, hidden_state)\n",
        "\n",
        "            # compute loss\n",
        "            loss = loss_fn(torch.squeeze(output), torch.squeeze(target_seq))\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # compute gradients and take optimizer step\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # update the data pointer\n",
        "            data_ptr += seq_len\n",
        "            n += 1\n",
        "\n",
        "            # if at end of data : break\n",
        "            if data_ptr + seq_len + 1 > data_size:\n",
        "                break\n",
        "\n",
        "        # print loss and save weights after every epoch\n",
        "        print(\"Epoch: {0} \\t Loss: {1:.8f}\".format(i_epoch, running_loss / n))\n",
        "        torch.save(rnn.state_dict(), save_path)\n",
        "\n",
        "        # sample / generate a text sequence after every epoch\n",
        "        data_ptr = 0\n",
        "        hidden_state = None\n",
        "\n",
        "        # random character from data to begin\n",
        "        rand_index = np.random.randint(data_size - 1)\n",
        "        input_seq = data[rand_index: rand_index + 1]\n",
        "\n",
        "        print(\"----------------------------------------\")\n",
        "        while True:\n",
        "            # forward pass\n",
        "            output, hidden_state = rnn(input_seq, hidden_state)\n",
        "\n",
        "            # construct categorical distribution and sample a character\n",
        "            output = F.softmax(torch.squeeze(output), dim=0)\n",
        "            dist = Categorical(output)\n",
        "            index = dist.sample()\n",
        "\n",
        "            # print the sampled character\n",
        "            print(ix_to_char[index.item()], end='')\n",
        "\n",
        "            # next input is\n",
        "            input_seq[0][0] = index.item()\n",
        "            data_ptr += 1\n",
        "\n",
        "            if data_ptr > op_seq_len:\n",
        "                break\n",
        "\n",
        "        print(\"\\n----------------------------------------\")"
      ],
      "metadata": {
        "id": "4-yAG2gICnTD"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train() # call the training loop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhoHPAItC4yZ",
        "outputId": "aa5a5107-c8d3-455b-d75c-d2d2e844aacf"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "Data has 1565532 characters, 27 unique\n",
            "----------------------------------------\n",
            "Epoch: 1 \t Loss: 1.71560139\n",
            "----------------------------------------\n",
            "ont dreally stoll someble bettamed i feil hot shated praally feeling wating exptirate or happelle ening best pith as i mave lre while i feel oucless with by thile wrepted her were theme sat i feel vinm\n",
            "----------------------------------------\n",
            "Epoch: 2 \t Loss: 1.48649242\n",
            "----------------------------------------\n",
            "to qrodush pilled at they its clacely discalrisged i blil that onerran is belazenent make him hers afe that moy brace i still feel bitteny i unching even for the capilet beantared whethere must people \n",
            "----------------------------------------\n",
            "Epoch: 3 \t Loss: 1.40871943\n",
            "----------------------------------------\n",
            "lblly feel apuse up there it of the foratm mirs at theneings inmodidastion about keeps i was sweeache is a caons feating and been feeling rany mess i feel like if its fine pustant and i s are offenday \n",
            "----------------------------------------\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}